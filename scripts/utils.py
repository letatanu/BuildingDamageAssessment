import os, json, inspect, runpy, argparse\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Subset\n\nfrom transformers import (\n    TrainingArguments,\n)\nfrom transformers.trainer_utils import get_last_checkpoint\nimport torch.distributed as dist\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config_file\", required=True, help=\"Path to dataset config .py file.\")\n    parser.add_argument(\"--data_root\", type=str, default=\"/data/FloodNet-Supervised_v1.0\")\n    parser.add_argument(\"--output_dir\", type=str, default=\"/working/runs/mask2former_floodnet\")\n    parser.add_argument(\"--model_name\", type=str, default=\"facebook/mask2former-swin-large-cityscapes-semantic\")\n    parser.add_argument(\"--num_classes\", type=int, default=10)\n    parser.add_argument(\"--ignore_index\", type=int, default=0)\n\n    parser.add_argument(\"--image_size\", type=int, default=512)\n    parser.add_argument(\"--batch_size\", type=int, default=8)\n    parser.add_argument(\"--eval_batch_size\", type=int, default=2)\n    parser.add_argument(\"--lr\", type=float, default=6e-5)\n    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n    parser.add_argument(\"--epochs\", type=int, default=300)\n    parser.add_argument(\"--seed\", type=int, default=42)\n    parser.add_argument(\"--fp16\", type=bool, default=False, help=\"use mixed precision\")\n    parser.add_argument(\"--save_total_limit\", type=int, default=2)\n    parser.add_argument(\"--warmup_ratio\", type=float, default=0.01)\n    parser.add_argument(\"--logging_steps\", type=int, default=50)\n    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n    parser.add_argument(\"--train_split\", type=str, default=\"train\")\n    parser.add_argument(\"--val_split\", type=str, default=\"val\")\n    parser.add_argument(\"--test_split\", type=str, default=\"test\")\n\n    parser.add_argument(\"--resume\", type=str, default=None,\n        help=\"Resume from a checkpoint: 'auto' to pick last in output_dir, a path, or 'none' to start fresh.\")\n    parser.add_argument(\"--overwrite_output_dir\", type=bool, default=False,\n        help=\"Allow training to start even if output_dir is non-empty.\")\n    \n    parser.add_argument(\"--val_fraction\", type=float, default=0.25, help=\"Use this fraction (0-1) of val images.\")\n    parser.add_argument(\"--val_seed\", type=int, default=1337, help=\"Deterministic subset seed.\")\n    parser.add_argument(\"--val_limit\", type=int, default=200,\n                    help=\"Evaluate on at most this many validation samples.\")\n    parser.add_argument(\"--evaluate\", type=bool, default=False,\n                        help=\"If set, only run evaluation using the model from --eval_from or best checkpoint in output_dir.\")\n    parser.add_argument(\"--eval_from\", type=str, default=None,\n                        help=\"Path to a model/checkpoint dir to evaluate. If omitted, the best checkpoint under --output_dir is used.\")\n    args = parser.parse_args()\n    \n    cfg = runpy.run_path(args.config_file)\n    args.data_root = cfg.get(\"data_root\", args.data_root)\n    args.num_classes = cfg.get(\"num_classes\", args.num_classes)\n    args.ignore_index = cfg.get(\"ignore_index\", args.ignore_index)\n    args.image_size = cfg[\"DATASET_KWARGS\"].get(\"image_size\", args.image_size)\n    args.batch_size = cfg.get(\"batch_size\", args.batch_size)\n    args.lr = cfg.get(\"lr\", args.lr)\n    args.weight_decay = cfg.get(\"weight_decay\", args.weight_decay)\n    args.epochs = cfg.get(\"num_epochs\", args.epochs)\n    args.fp16 = cfg.get(\"fp16\", args.fp16)\n    args.save_total_limit = cfg.get(\"save_total_limit\", args.save_total_limit)\n    args.warmup_ratio = cfg.get(\"warmup_ratio\", args.warmup_ratio)\n    args.logging_steps = cfg.get(\"logging_steps\", args.logging_steps)\n    args.gradient_accumulation_steps = cfg.get(\"gradient_accumulation_steps\", args.gradient_accumulation_steps) \n    args.output_dir = cfg.get(\"output_dir\", args.output_dir)\n    args.model_name = cfg.get(\"model_name\", args.model_name)\n    args.resume = cfg.get(\"resume\", args.resume)\n    args.overwrite_output_dir = cfg.get(\"overwrite_output_dir\", args.overwrite_output_dir)\n    args.eval_from = cfg.get(\"eval_from\", args.eval_from)\n    args.eval_limit = cfg.get(\"eval_limit\", args.val_limit)\n    args.evaluate = cfg.get(\"evaluate\", args.evaluate)\n    args.train_split = cfg.get(\"train_split\", args.train_split)\n    args.val_split = cfg.get(\"val_split\", args.val_split)\n    args.test_split = cfg.get(\"test_split\", args.test_split)\n    return args\n\ndef setup_devices_autodetect():\n    \"\"\"\n    If launched via torchrun (WORLD_SIZE>1 and a rank set) -> DDP.\n    Else -> single process, single GPU (scrub DDP env so Accelerate stays off).\n    \"\"\"\n    gpu_count = torch.cuda.device_count()\n    env_world = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n    has_rank = (\"LOCAL_RANK\" in os.environ) or (\"RANK\" in os.environ)\n    ddp_env = env_world > 1 and has_rank\n\n    if ddp_env:\n        local_rank = int(os.environ.get(\"LOCAL_RANK\", os.environ.get(\"RANK\", \"0\")))\n        if torch.cuda.is_available() and gpu_count > 0:\n            torch.cuda.set_device(local_rank % gpu_count)\n        mode = \"ddp\"\n        world_size = env_world\n    else:\n        for k in (\"LOCAL_RANK\",\"RANK\",\"WORLD_SIZE\",\"MASTER_ADDR\",\"MASTER_PORT\",\n                  \"SLURM_PROCID\",\"SLURM_NTASKS\",\"PMI_RANK\",\"PMI_SIZE\",\"ACCELERATE_USE_DISTRIBUTED\"):\n            os.environ.pop(k, None)\n        os.environ[\"ACCELERATE_USE_DISTRIBUTED\"] = \"false\"\n        if torch.cuda.is_available() and gpu_count > 0:\n            torch.cuda.set_device(0)\n        mode = \"single\"\n        local_rank = 0\n        world_size = 1\n    return mode, local_rank, world_size\n\n\ndef take_first_n(dataset, n: int):\n    if n is None or n <= 0:\n        return dataset\n    from torch.utils.data import Subset\n    return Subset(dataset, list(range(min(n, len(dataset)))))\n\ndef ddp_barrier_safe():\n    if dist.is_available() and dist.is_initialized():\n        try:\n            dist.barrier(device_ids=[torch.cuda.current_device()])  # PyTorch >=2.0\n        except TypeError:\n            dist.barrier()\ndef safe_training_args(**kwargs):\n    sig = inspect.signature(TrainingArguments.__init__)\n    allowed = set(sig.parameters.keys()) - {\"self\"}\n    filtered = {k: v for k, v in kwargs.items() if k in allowed}\n    return TrainingArguments(**filtered)\n\n\ndef make_val_subset(dataset, limit: int | None = None, fraction: float | None = None, seed: int = 42):\n    n = len(dataset)\n    if limit is None and (fraction is None or fraction <= 0 or fraction > 1):\n        return dataset\n    if fraction is not None and limit is None:\n        limit = max(1, int(round(n * fraction)))\n    # deterministic pick: shuffle indices with a fixed seed, then take the head\n    g = np.random.default_rng(seed)\n    indices = np.arange(n)\n    g.shuffle(indices)\n    indices = indices[:limit]\n    indices.sort()  # keep loader cache-friendly ordering\n    return Subset(dataset, indices.tolist())\n\ndef choose_resume_checkpoint(resume_arg: str, out_dir: str) -> str | bool:\n    if resume_arg is None or str(resume_arg).lower() == \"none\":\n        return False\n    if resume_arg == \"auto\":\n        last = get_last_checkpoint(out_dir)\n        if last is not None:\n            return last\n        p = Path(out_dir)\n        if not p.exists():\n            return False\n        cks = sorted(\n            [d for d in p.iterdir() if d.is_dir() and d.name.startswith(\"checkpoint-\")],\n            key=lambda d: int(d.name.split(\"-\")[-1]) if d.name.split(\"-\")[-1].isdigit() else -1\n        )\n        return str(cks[-1]) if cks else False\n    p = Path(resume_arg)\n    return str(p) if p.exists() else False\n\ndef discover_best_model_dir(base_dir: str) -> str:\n    \"\"\"\n    Resolve a directory that can be passed to from_pretrained():\n      1) If base_dir already contains weights, return it.\n      2) If trainer_state.json has best_model_checkpoint, return that.\n      3) Else fall back to the last checkpoint in base_dir.\n      4) Else return base_dir (best effort).\n    \"\"\"\n    p = Path(base_dir)\n\n    # Direct weights?\n    if (p / \"pytorch_model.bin\").exists() or (p / \"model.safetensors\").exists():\n        return str(p)\n\n    # trainer_state.json â†’ best_model_checkpoint\n    ts = p / \"trainer_state.json\"\n    if ts.exists():\n        try:\n            state = json.loads(ts.read_text())\n            best = state.get(\"best_model_checkpoint\")\n            if best and Path(best).exists():\n                return best\n        except Exception:\n            pass\n\n    # last checkpoint\n    last = get_last_checkpoint(str(p))\n    if last:\n        return last\n\n    return str(p)\n\ndef compute_mIoU(eval_preds, num_classes: int, ignore_index: int = 255):\n    \"\"\"\n    eval_preds: (logits, labels)\n      - logits: np.ndarray [N, C, h, w]  (often downsampled, e.g., 128x128)\n      - labels: np.ndarray [N, H, W]     (your target size, e.g., 512x512)\n    \"\"\"\n\n    logits, labels = eval_preds\n\n    # to torch\n    logits_t = torch.from_numpy(logits)       # [N,C,h,w]\n    labels_t = torch.from_numpy(labels)       # [N,H,W]\n    if labels_t.dtype != torch.long:\n        labels_t = labels_t.long()\n\n    # upsample logits to label size\n    N, C, h, w = logits_t.shape\n    H, W = labels_t.shape[-2], labels_t.shape[-1]\n    if (h, w) != (H, W):\n        logits_t = F.interpolate(logits_t, size=(H, W), mode=\"bilinear\", align_corners=False)\n\n    preds = logits_t.argmax(dim=1)            # [N,H,W]\n    preds_np = preds.cpu().numpy()\n    labels_np = labels_t.cpu().numpy()\n\n    # confusion accumulators\n    tp = np.zeros(num_classes, dtype=np.int64)\n    fp = np.zeros(num_classes, dtype=np.int64)\n    fn = np.zeros(num_classes, dtype=np.int64)\n\n    for p, g in zip(preds_np, labels_np):\n        mask = (g != ignore_index)\n        if not np.any(mask):\n            continue\n        p = p[mask]\n        g = g[mask]\n        for cls in range(num_classes):\n            p_c = (p == cls)\n            g_c = (g == cls)\n            tp[cls] += np.logical_and(p_c, g_c).sum()\n            fp[cls] += np.logical_and(p_c, ~g_c).sum()\n            fn[cls] += np.logical_and(~p_c, g_c).sum()\n\n    iou = np.zeros(num_classes, dtype=np.float64)\n    for c in range(num_classes):\n        denom = tp[c] + fp[c] + fn[c]\n        iou[c] = float(tp[c]) / denom if denom > 0 else float(\"nan\")\n\n    miou = float(np.nanmean(iou))\n    macc = float((tp / np.maximum(tp + fn, 1)).mean())\n\n    metrics = {\"mIoU\": miou, \"mAcc\": macc}\n    for c, v in enumerate(iou):\n        metrics[f\"IoU_{c}\"] = 0.0 if np.isnan(v) else float(v)\n    return metrics\n\ndef _to_list_class_labels(x):\n    \"\"\"Return a python list[int] from list/tuple/Tensor/None.\"\"\"\n    if x is None:\n        return []\n    if isinstance(x, (list, tuple)):\n        return [int(v) for v in x]\n    if isinstance(x, torch.Tensor):\n        return x.detach().cpu().flatten().to(torch.long).tolist()\n    raise TypeError(f\"Unsupported class_labels type: {type(x)}\")\n\ndef _to_list_masks(x):\n    \"\"\"Return a list[Tensor(H,W,bool)] from list/tuple/Tensor/None.\"\"\"\n    if x is None:\n        return []\n    out = []\n    if isinstance(x, (list, tuple)):\n        it = x\n    elif isinstance(x, torch.Tensor):\n        # shapes: [N,H,W] or [H,W]\n        if x.ndim == 2:\n            it = [x]\n        elif x.ndim == 3:\n            it = [x[i] for i in range(x.shape[0])]\n        else:\n            raise ValueError(f\"mask_labels tensor must be 2D/3D, got {x.shape}\")\n    else:\n        raise TypeError(f\"Unsupported mask_labels type: {type(x)}\")\n\n    for m in it:\n        mt = torch.as_tensor(m)\n        if mt.ndim == 3 and mt.shape[0] == 1:  # squeeze channel if [1,H,W]\n            mt = mt[0]\n        if mt.ndim != 2:\n            raise ValueError(f\"Each mask must be 2D, got {mt.shape}\")\n        out.append(mt.to(dtype=torch.bool, copy=False))\n    return out\n\ndef _masks_to_semantic(class_labels_list, mask_list, ignore_index=0):\n    \"\"\"\n    Build an [H,W] long tensor with class ids. Later masks override earlier ones.\n    \"\"\"\n    if len(mask_list) == 0:\n        raise ValueError(\"mask_labels is empty; cannot reconstruct semantic map\")\n\n    H, W = mask_list[0].shape[-2], mask_list[0].shape[-1]\n    canvas = torch.full((H, W), int(ignore_index), dtype=torch.long)\n    # last mask wins (stable & simple)\n    for c, m in zip(class_labels_list, mask_list):\n        canvas[m] = int(c)\n    return canvas